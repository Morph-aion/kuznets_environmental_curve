---
title: "Temporal analysis: The US Case Study"
execute:
  cache: false
---

{{< include _setup.qmd >}}
{{< include _footnotes.qmd >}}

```{python}
#| label: setup-english-mapping-usa
#| echo: false

# MAPPING VARIABLES (Fix pour Ã©viter NameError si _setup.qmd est en franÃ§ais)
# On mappe les variables globales du setup vers des noms anglais
if 'ANNEE_DEBUT_DATA' in locals():
    START_YEAR_DATA = ANNEE_DEBUT_DATA
if 'ANNEE_FIN' in locals():
    END_YEAR = ANNEE_FIN
    
# Calcul de sÃ©curitÃ© pour GDP_MEAN_USA si pas dÃ©fini
if 'GDP_MEAN_USA' not in locals() and 'df_usa' in locals() and not df_usa.empty:
    GDP_MEAN_USA = np.log(df_usa['gdp_pc']).mean()
elif 'GDP_MEAN_USA' not in locals():
    GDP_MEAN_USA = 0 # Fallback pour Ã©viter crash
```

```{python}
#| label: estimation-ardl-v1
#| output: false

if DATA_LOADED and not df_usa.empty:
    m_ar = smf.ols('log_co2 ~ lag_log_co2', data=df_usa).fit()
    m_ardl_lin = smf.ols('log_co2 ~ lag_log_co2 + log_gdp_c', data=df_usa).fit()
    m_ardl_quad = smf.ols('log_co2 ~ lag_log_co2 + log_gdp_c + I(log_gdp_c**2)', data=df_usa).fit()
    m_ardl_cub = smf.ols('log_co2 ~ lag_log_co2 + log_gdp_c + I(log_gdp_c**2) + I(log_gdp_c**3)', data=df_usa).fit(cov_type='HAC', cov_kwds={'maxlags': 3})
    
    RHO = m_ardl_cub.params.get('lag_log_co2', 0)
    BETA3_USA = m_ardl_cub.params.get('I(log_gdp_c ** 3)', 0)
    PVAL3_USA = m_ardl_cub.pvalues.get('I(log_gdp_c ** 3)', 1)
    
    HALFLIFE = np.log(0.5) / np.log(RHO) if 0 < RHO < 1 else np.nan
```

## Why the United States?

- First historical cumulative emitter (~25% of global CO$_2$ since 1850)
- Mature post-industrial economy ($76,000/capita)
- Complete territorial emissions data (`{python} START_YEAR_DATA`-`{python} END_YEAR`)

## Stationarity tests (ADF[^adf])

```{python}
#| echo: false

if DATA_LOADED and not df_usa.empty:
    from statsmodels.tsa.stattools import adfuller
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 1. ADF TESTS WITH FULL DIAGNOSTIC
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # Levels tests
    adf_co2 = adfuller(df_usa['log_co2'], autolag='AIC', regression='ct')  # with constant + trend
    adf_gdp = adfuller(df_usa['log_gdp'], autolag='AIC', regression='ct')
    
    # First difference tests
    adf_co2_diff = adfuller(df_usa['log_co2'].diff().dropna(), autolag='AIC', regression='c')  # constant only
    adf_gdp_diff = adfuller(df_usa['log_gdp'].diff().dropna(), autolag='AIC', regression='c')
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 2. AUTOMATIC INTERPRETATION FUNCTION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def interprete_adf(pval_levels, pval_diff, threshold=0.05):
        """
        Returns I(0), I(1), or 'Ambiguous' based on p-values
        """
        if pval_levels < threshold:
            return "I(0)", "âœ… Stationary in levels"
        elif pval_diff < threshold:
            return "I(1)", "âœ… Non-stationary in levels, stationary in difference"
        else:
            return "Ambiguous", "âš ï¸ Inconclusive tests (possibly I(2) or structural break)"
    
    # Application
    concl_co2, msg_co2 = interprete_adf(adf_co2[1], adf_co2_diff[1])
    concl_gdp, msg_gdp = interprete_adf(adf_gdp[1], adf_gdp_diff[1])
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 3. TABLE DISPLAY
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    display(Markdown(f"""
**Stationarity tests (ADF with constant + trend)**

| Variable | ADF (levels) | p-value | ADF (diff.) | p-value | **Conclusion** |
|----------|--------------|---------|-------------|---------|----------------|
| $\\ln(\\text{{CO}}_2)$ | {adf_co2[0]:.2f} | **{adf_co2[1]:.3f}** | {adf_co2_diff[0]:.2f} | {adf_co2_diff[1]:.3f} | **{concl_co2}** |
| $\\ln(\\text{{GDP}})$ | {adf_gdp[0]:.2f} | **{adf_gdp[1]:.3f}** | {adf_gdp_diff[0]:.2f} | {adf_gdp_diff[1]:.3f} | **{concl_gdp}** |

*Reminder: Hâ‚€ = "presence of a unit root (non-stationary)". If p < 0.05, we reject Hâ‚€ â†’ stationary series (I(0)).*
"""))
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 4. CONDITIONAL INTERPRETATION AND WARNING
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    if concl_co2 == concl_gdp == "I(1)":
        # IDEAL CASE
        display(Markdown("""
**âœ… Interpretation**: Both series are **I(1)** (non-stationary in levels, stationary in first difference).

**Methodological implication**: The **ARDL** model is appropriate. If the series are cointegrated, the ARDL will capture the long-run relationship. Otherwise, it will correctly describe the short-run dynamics.
"""))
    
    elif concl_co2 == "I(0)" and concl_gdp == "I(0)":
        # BOTH I(0)
        display(Markdown("""
**âœ… Interpretation**: Both series are **I(0)** (stationary in levels).

**Methodological implication**: The ARDL model remains valid. A static model (simple OLS) could also be estimated without lags, but ARDL will better capture adjustment dynamics.
"""))
    
    elif (concl_co2 == "I(1)" and concl_gdp == "I(0)") or (concl_co2 == "I(0)" and concl_gdp == "I(1)"):
        # PROBLEMATIC CASE: I(0)/I(1) ASYMMETRY
        display(Markdown(f"""
**âš ï¸ DIAGNOSTIC: Integration asymmetry detected**

- $\\ln(\\text{{CO}}_2)$ : **{concl_co2}** ({msg_co2})
- $\\ln(\\text{{GDP}})$ : **{concl_gdp}** ({msg_gdp})

**Theoretical problem**: Regressing an I(1) variable on an I(0) variable (or vice versa) can lead to biased results if the series are not cointegrated.

**Adopted solution**: The **ARDL model remains valid** in this case (Pesaran et al., 2001):
- ARDL accepts a mix of I(0) and I(1) regressors
- Condition: no variable must be I(2) (which is verified here)
- Recommended robustness test: **Bounds Test** for cointegration (not performed in this report)

**Cautious interpretation**: Long-term elasticities should be interpreted with reserve. Temporal inertia (coefficient $\\rho$) remains reliable.
"""))
    
    else:
        # AMBIGUOUS CASE
        display(Markdown(f"""
**âš ï¸ DIAGNOSTIC: Inconclusive ADF tests**

- $\\ln(\\text{{CO}}_2)$ : {concl_co2}
- $\\ln(\\text{{GDP}})$ : {concl_gdp}

**Possible causes**:
1. **Low power of the test** (sample T=30 too small)
2. **Structural break** (e.g., 2008 crisis, COVID-2020) not taken into account
3. **I(2) series** (unlikely for GDP and COâ‚‚)

**Adopted solution**: We proceed with the **ARDL** model, but acknowledge that stationarity tests are inconclusive on a small sample.
"""))
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 5. SUPPLEMENTARY TEST: ADF WITHOUT TREND
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    if concl_gdp == "I(0)" and adf_gdp[1] < 0.10:  # If borderline result
        adf_gdp_no_trend = adfuller(df_usa['log_gdp'], autolag='AIC', regression='c')
        
        display(Markdown(f"""
---

**ğŸ” Robustness test (ln(GDP) with alternative specification)**

The p-value close to the threshold (p={adf_gdp[1]:.3f}) may be sensitive to test specification (with/without trend).

| Specification | ADF | p-value | Conclusion |
|---------------|-----|---------|------------|
| Constant + Trend | {adf_gdp[0]:.2f} | {adf_gdp[1]:.3f} | {concl_gdp} |
| Constant only | {adf_gdp_no_trend[0]:.2f} | {adf_gdp_no_trend[1]:.3f} | {"I(0)" if adf_gdp_no_trend[1] < 0.05 else "I(1)"} |

**Interpretation**: {
    "Both specifications converge â†’ robust result." 
    if (adf_gdp[1] < 0.05) == (adf_gdp_no_trend[1] < 0.05)
    else "**Divergence detected** â†’ ln(GDP) is likely **trend-stationary** (I(0) around a deterministic trend, not I(1)). The ARDL model remains valid."
}
"""))
```

**Conclusion**: The series are $I(1) \rightarrow$ **ARDL** model appropriate.

## Estimation of candidate models {#sec-estimation-usa}

We estimate **four ARDL specifications**[^ardl] to capture the dynamic relationship between $\text{GDP}$ and $\text{CO}_2$:

1. **AR(1)**: Pure autoregressive model (without $\text{GDP}$) â€” minimal benchmark
2. **Linear ARDL**: Adds $\ln(\text{GDP})$ as regressor
3. **Quadratic ARDL**: Adds $[\ln(\text{GDP})]^2$
4. **Cubic ARDL**: Adds $[\ln(\text{GDP})]^3$ (rebound test)

```{python}
#| label: estimation-ardl-v2
#| output: false

if DATA_LOADED and not df_usa.empty:
    # Estimation with Newey-West standard errors (3 lags)
    kwds = {'maxlags': 3}
    m_ar = smf.ols('log_co2 ~ lag_log_co2', data=df_usa).fit(cov_type='HAC', cov_kwds=kwds)
    m_ardl_lin = smf.ols('log_co2 ~ lag_log_co2 + log_gdp_c', data=df_usa).fit(cov_type='HAC', cov_kwds=kwds)
    m_ardl_quad = smf.ols('log_co2 ~ lag_log_co2 + log_gdp_c + I(log_gdp_c**2)', data=df_usa).fit(cov_type='HAC', cov_kwds=kwds)
    m_ardl_cub = smf.ols('log_co2 ~ lag_log_co2 + log_gdp_c + I(log_gdp_c**2) + I(log_gdp_c**3)', data=df_usa).fit(cov_type='HAC', cov_kwds=kwds)
```

```{python}
#| label: tbl-ardl
#| tbl-cap: "ARDL Models â€” United States (1990-2020)"
#| echo: false

from IPython.display import display, Markdown
import statsmodels.formula.api as smf
import numpy as np

# Safety check
if 'DATA_LOADED' in locals() and DATA_LOADED and not df_usa.empty:
    
    # 1. Formatting function
    def fmt_ardl(model, var):
        if var not in model.params.index:
            return "â€”"
        c = model.params[var]
        s = model.bse[var]
        p = model.pvalues[var]
        stars = '***' if p < 0.01 else '**' if p < 0.05 else '*' if p < 0.1 else ''
        stars_escaped = stars.replace('*', '\\*') # Fix for markdown table
        return f"{c:.4f}{stars_escaped} ({s:.4f})"
    
    # 2. Estimation (already done above, but variables must be available)
    # Using the models from previous block
    
    # 3. Table construction
    rows = [
        ('$\\ln(\\text{CO}_2)_{t-1}$', 'lag_log_co2'),
        ('$\\ln(\\text{GDP})_t$', 'log_gdp_c'),
        ('$[\\ln(\\text{GDP})]^2$', 'I(log_gdp_c ** 2)'), 
        ('$[\\ln(\\text{GDP})]^3$', 'I(log_gdp_c ** 3)'),
    ]
    
    table = "| Variable | AR(1) | Linear ARDL | Quad. ARDL | Cubic ARDL |\n"
    table += "|----------|-------|-------------|------------|------------|\n"
    
    for label, var in rows:
        table += f"| {label} | {fmt_ardl(m_ar, var)} | {fmt_ardl(m_ardl_lin, var)} | "
        table += f"{fmt_ardl(m_ardl_quad, var)} | {fmt_ardl(m_ardl_cub, var)} |\n"
    
    table += f"| **RÂ²** | {m_ar.rsquared:.3f} | {m_ardl_lin.rsquared:.3f} | "
    table += f"{m_ardl_quad.rsquared:.3f} | {m_ardl_cub.rsquared:.3f} |\n"
    table += f"| **AIC** | {m_ar.aic:.1f} | {m_ardl_lin.aic:.1f} | "
    table += f"{m_ardl_quad.aic:.1f} | {m_ardl_cub.aic:.1f} |\n"
    
    display(Markdown(table))
    display(Markdown("*Newey-West standard errors (3 lags). $^{*}p<0.1$, $^{{**}}p<0.05$, $^{{***}}p<0.01$*"))

    # 4. Calculations for observation text
    models_aic = [m_ar.aic, m_ardl_lin.aic, m_ardl_quad.aic, m_ardl_cub.aic]
    model_names = ['AR', 'Linear', 'Quadratic', 'Cubic']
    best_aic_model = model_names[np.argmin(models_aic)]
    
    try:
        p_val_cub = m_ardl_cub.pvalues['I(log_gdp_c ** 3)']
    except KeyError:
        p_val_cub = m_ardl_cub.pvalues.get('I(log_gdp_c**3)', 1.0)

    # 5. Dynamic observation text
    display(Markdown(f"""
***

**Preliminary observation**: 
- The cubic model has the best RÂ² ({m_ardl_cub.rsquared:.3f}), 
  but $\\beta_3$ is not significant (p = {p_val_cub:.3f}).
- AIC favors the **{best_aic_model}** model.

**â†’ We must arbitrate between statistical significance and parsimony. 
Out-of-sample validation will decide.**
    """))
else:
    print("Data not loaded.")
```

## Out-of-sample validation {#sec-validation-usa}

### Protocol

**Why a simple train/test split (and not expanding window)?**

Expanding window (Section 3.3.2.2) is theoretically superior for time series, 
but with only **30 observations**, an expanding window of 10 iterations would result in:

- 1st iteration: 21 training observations (1990-2010)
- Last iteration: 30 observations (1990-2019)

â†’ Too few data points to robustly estimate a cubic ARDL model (4 parameters).

**Adopted compromise**: Fixed train/test split (1990-2015 / 2016-2020) with **5 test observations**, 
consistent with the 80/20 rule of thumb.

```{python}
#| label: validation-setup
#| output: false

train = df_usa[df_usa['year'] <= 2015].copy()
test = df_usa[df_usa['year'] > 2015].copy()

# Re-estimation on train only
kwds = {'maxlags': 3}
m_ar_train = smf.ols('log_co2 ~ lag_log_co2', data=train).fit(cov_type='HAC', cov_kwds=kwds)
m_lin_train = smf.ols('log_co2 ~ lag_log_co2 + log_gdp_c', data=train).fit(cov_type='HAC', cov_kwds=kwds)
m_quad_train = smf.ols('log_co2 ~ lag_log_co2 + log_gdp_c + I(log_gdp_c**2)', data=train).fit(cov_type='HAC', cov_kwds=kwds)
m_cub_train = smf.ols('log_co2 ~ lag_log_co2 + log_gdp_c + I(log_gdp_c**2) + I(log_gdp_c**3)', data=train).fit(cov_type='HAC', cov_kwds=kwds)
```

### Comparison with benchmarks

```{python}
#| label: benchmarks-calcul
#| output: false

from sklearn.metrics import mean_squared_error, mean_absolute_error
from statsmodels.tsa.arima.model import ARIMA

y_test = test['log_co2']

# 1. ARDL (4 versions)
y_pred_ar = m_ar_train.predict(test)
y_pred_lin = m_lin_train.predict(test)
y_pred_quad = m_quad_train.predict(test)
y_pred_cub = m_cub_train.predict(test)

# 2. Naive benchmarks
y_pred_rw = test['lag_log_co2']  # Random Walk
y_pred_ma = df_usa['log_co2'].rolling(3).mean().shift(1).loc[test.index]

# 3. ARIMA(1,1,1)
# Warning: statsmodels ARIMA requires stationary data or proper differencing parameter d
arima_model = ARIMA(train['log_co2'], order=(1,1,1)).fit()
y_pred_arima = arima_model.forecast(steps=len(test))

# Metrics calculation
def metrics(name, y_true, y_pred):
    return {
        'Model': name,
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
        'MAE': mean_absolute_error(y_true, y_pred),
        'MAPE': np.mean(np.abs((np.exp(y_true) - np.exp(y_pred)) / np.exp(y_true))) * 100
    }

results = [
    metrics('Random Walk', y_test, y_pred_rw),
    metrics('Moving Average', y_test, y_pred_ma),
    metrics('ARIMA(1,1,1)', y_test, y_pred_arima),
    metrics('ARDL â€” AR(1)', y_test, y_pred_ar),
    metrics('ARDL â€” Linear', y_test, y_pred_lin),
    metrics('ARDL â€” Quadratic', y_test, y_pred_quad),
    metrics('ARDL â€” Cubic', y_test, y_pred_cub),
]

df_perf = pd.DataFrame(results).sort_values('RMSE')
```

```{python}
#| label: tbl-performance-oos
#| tbl-cap: "Out-of-sample performance (2016-2020)"

from IPython.display import display, Markdown

display(Markdown(df_perf.to_markdown(index=False, floatfmt=('.0s', '.4f', '.4f', '.2f'))))
```

```{python}
#| label: fig-comparaison-modeles
#| fig-cap: "Visual comparison of out-of-sample predictions"
#| fig-width: 12
#| fig-height: 5

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Plot 1: Time series
axes[0].plot(test['year'], np.exp(y_test), 'ko-', linewidth=2, markersize=8, label='Observed')
axes[0].plot(test['year'], np.exp(y_pred_rw), 's--', alpha=0.6, label='Random Walk')
axes[0].plot(test['year'], np.exp(y_pred_arima), '^--', alpha=0.6, label='ARIMA')
axes[0].plot(test['year'], np.exp(y_pred_quad), 'r-', linewidth=2, label='ARDL Quadratic')
axes[0].set_xlabel("Year")
axes[0].set_ylabel("COâ‚‚ per capita (tons)")
axes[0].set_title("A. Predictions vs Reality (2016-2020)")
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Plot 2: Error distribution (boxplot)
errors = pd.DataFrame({
    'RW': y_test - y_pred_rw,
    'ARIMA': y_test - y_pred_arima,
    'Quad': y_test - y_pred_quad,
})
errors.boxplot(ax=axes[1])
axes[1].axhline(0, color='red', linestyle='--')
axes[1].set_ylabel("Prediction error (log)")
axes[1].set_title("B. Error distribution")
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Model selection decision

```{python}
#| echo: false

from IPython.display import display, Markdown

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. RETRIEVE METRICS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_rmse(model_name):
    try:
        return df_perf[df_perf['Model'] == model_name]['RMSE'].values[0]
    except IndexError:
        return np.nan

rmse_lin = get_rmse('ARDL â€” Linear')
rmse_quad = get_rmse('ARDL â€” Quadratic')
rmse_cub = get_rmse('ARDL â€” Cubic')
rmse_rw = get_rmse('Random Walk')

# Relative calculations
diff_quad_pct = (rmse_quad - rmse_lin) / rmse_lin * 100
gain_vs_rw = (rmse_rw - min(rmse_lin, rmse_quad)) / rmse_rw * 100

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. SIGNIFICANCE ANALYSIS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

try:
    pval_b2 = m_ardl_quad.pvalues.get('I(log_gdp_c ** 2)', m_ardl_quad.pvalues.get('I(log_gdp_c**2)', 1.0))
    pval_b3 = m_ardl_cub.pvalues.get('I(log_gdp_c ** 3)', m_ardl_cub.pvalues.get('I(log_gdp_c**3)', 1.0))
except NameError:
    pval_b2, pval_b3 = 1.0, 1.0

is_quad_sig = pval_b2 < 0.10
is_cub_sig = pval_b3 < 0.10

txt_sig_b2 = "**Significant**" if is_quad_sig else "Not significant"
txt_sig_b3 = "**Significant**" if is_cub_sig else "Not significant"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. DECISION LOGIC
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if is_quad_sig:
    model_retained = "Quadratic ARDL"
    justification_b2 = f"The coefficient $\\beta_2$ is significant (p={pval_b2:.3f}), validating the inverted U-curve hypothesis (EKC)."
    tradeoff_txt = "The slight loss in RMSE is justified by the better theoretical specification."
else:
    model_retained = "Linear ARDL (Statistical) / Quadratic (Theoretical)"
    justification_b2 = f"The coefficient $\\beta_2$ is not statistically significant (p={pval_b2:.3f}), suggesting that over this period (1990-2020), the relationship is mainly linear (EKC downward phase already begun)."
    tradeoff_txt = "The linear model is more performant and parsimonious. However, we retain the quadratic model for structural analysis (turning point) for consistency with the cross-sectional analysis."

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. MARKDOWN REPORT GENERATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

display(Markdown(f"""

**Out-of-sample validation results:**

1. **Best global model**: {df_perf.iloc[0]['Model']} (RMSE = {df_perf.iloc[0]['RMSE']:.4f})
2. **Gain vs naive benchmark**: {gain_vs_rw:+.1f}%

**Detailed comparison of specifications:**

| Criteria | Linear ARDL | Quadratic ARDL | Cubic ARDL |
|:--------|:--------------|:-----------------|:-------------|
| **RMSE (Test)** | **{rmse_lin:.4f}** | {rmse_quad:.4f} ({diff_quad_pct:+.1f}%) | {rmse_cub:.4f} |
| **Key coefficient** | â€” | $\\beta_2$: p={pval_b2:.3f} | $\\beta_3$: p={pval_b3:.3f} |
| **Status** | â€” | {txt_sig_b2} | {txt_sig_b3} |
| **AIC** | **{m_ardl_lin.aic:.1f}** | {m_ardl_quad.aic:.1f} | {m_ardl_cub.aic:.1f} |

**RETAINED MODEL: {model_retained}**

**Justification for decision:**

1.  **Performance**: The Linear model offers the best predictive performance (lowest RMSE) and best parsimony (lowest AIC).
    
2.  **EKC Test ($\\{"beta"}_2$)**: {justification_b2}

3.  **Rebound Test ($\\{"beta"}_3$)**: The cubic term is rejected (p={pval_b3:.3f}), ruling out the N-curve hypothesis at this stage.

**Methodological conclusion**: 
{tradeoff_txt}
"""))
```

## Diagnostics of the retained model {#sec-diagnostics-usa}

```{python}
#| label: diagnostics-modele-final
#| fig-cap: "Diagnostics of the quadratic ARDL model"
#| fig-width: 12
#| fig-height: 8

# Using the model estimated on full data
m_final = m_ardl_quad

fig, axes = plt.subplots(2, 2, figsize=(12, 8))

# 1. Residuals vs time
axes[0,0].plot(df_usa['year'], m_final.resid, 'o-')
axes[0,0].axhline(0, color='red', linestyle='--')
axes[0,0].set_xlabel("Year")
axes[0,0].set_ylabel("Residuals")
axes[0,0].set_title("A. Temporal Residuals")
axes[0,0].grid(True, alpha=0.3)

# 2. Residuals vs fitted
axes[0,1].scatter(m_final.fittedvalues, m_final.resid)
axes[0,1].axhline(0, color='red', linestyle='--')
axes[0,1].set_xlabel("Fitted values")
axes[0,1].set_ylabel("Residuals")
axes[0,1].set_title("B. Residuals vs Predictions")
axes[0,1].grid(True, alpha=0.3)

# 3. QQ-plot (normality)
from scipy import stats
stats.probplot(m_final.resid, dist="norm", plot=axes[1,0])
axes[1,0].set_title("C. Q-Q Plot (Normality Test)")

# 4. ACF of residuals (autocorrelation)
from statsmodels.graphics.tsaplots import plot_acf
plot_acf(m_final.resid, lags=10, ax=axes[1,1])
axes[1,1].set_title("D. Residual Autocorrelation")

plt.tight_layout()
plt.show()
```

```{python}
#| echo: false
from statsmodels.stats.stattools import jarque_bera, durbin_watson
from IPython.display import display, Markdown

# 1. Statistics calculation
jb_stat, jb_pval, _, _ = jarque_bera(m_final.resid)
dw_stat = durbin_watson(m_final.resid)

# 2. Verdict messages
jb_verdict = "Normal residuals" if jb_pval > 0.05 else "Non-normality detected"
dw_verdict = "No autocorrelation" if 1.5 < dw_stat < 2.5 else "Residual autocorrelation"

# 3. Display
display(Markdown(f"""
**Specification tests**:

| Test | Statistic | p-value | Verdict |
|------|-------------|---------|---------|
| **Jarque-Bera** (normality) | {jb_stat:.2f} | {jb_pval:.3f} | {jb_verdict} |
| **Durbin-Watson** (autocorrelation) | {dw_stat:.2f} | â€” | {dw_verdict} |

**Diagnostics conclusion**: The quadratic ARDL model satisfies the basic assumptions 
of linear regression. Residuals do not show any exploitable systematic pattern.
"""))
```

## Economic interpretation {#sec-interpretation-usa}

### Inertia and half-life

```{python}
#| echo: false

rho = m_final.params['lag_log_co2']

if 0 < rho < 0.99:
    halflife = -np.log(0.5) / np.log(rho)
    display(Markdown(f"""
**Inertia coefficient**: $\\rho = {rho:.3f}$ | **Half-life**: ${halflife:.1f}$ years

The coefficient $\\rho$ measures emissions persistence. A half-life of ${halflife:.1f}$ years 
means that it takes **${int(halflife*2)}+$ years** for a climate policy to have 
a lasting structural effect (well beyond a 4-5 year mandate).
"""))

elif rho >= 0.99:
    display(Markdown(f"""
**Inertia coefficient**: $\\rho = {rho:.3f}$ (close to unit root)

Extreme persistence of emissions â†’ Shocks are quasi-permanent. With only 30 observations, 
the estimator is biased upwards (Nickell bias). The true value is likely $\\rho \\approx 0.90â€“0.95$.
"""))

else:
    display(Markdown(f"**Abnormal coefficient**: $\\rho = {rho:.3f}$ (should be between 0 and 1)."))
```

### Long-term elasticity

```{python}
#| echo: false

rho = m_final.params['lag_log_co2']
beta_pib = m_final.params['log_gdp_c']
beta_pib2 = m_final.params['I(log_gdp_c ** 2)']

log_gdp_mean = df_usa['log_gdp'].mean()
log_gdp_mean_c = 0
gdp_mean_level = int(np.exp(log_gdp_mean))

elasticity_SR = beta_pib + 2 * beta_pib2 * log_gdp_mean_c

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Validity test: Ï < 0.99 ?
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if rho >= 0.99:
    # CASE 1: LT Elasticity not calculable
    effect_SR_pct = abs(elasticity_SR)
    verb_SR = "reduces" if elasticity_SR < 0 else "increases"
    
    display(Markdown(f"""
**US Average GDP**: ${gdp_mean_level:,}$ \$/capita (PPP 2017)

**Long-term elasticity not calculable** ($\\rho = {rho:.3f} \\approx 1$)

With $\\rho \\geq 1$, the long-term elasticity formula $\\varepsilon_{{LT}} = \\frac{{\\beta_{{\\text{{GDP}}}}}}{{1 - \\rho}}$ mathematically diverges. Possible causes: 
small sample ($n=30$), cointegration issues, or post-2010 structural instability.

**Short-term elasticity only**: $\\varepsilon_{{ST}} = {elasticity_SR:.3f}$  
â†’ +1\\% GDP **{verb_SR}** emissions by **{effect_SR_pct:.2f}\\%** (year 1).

**Conclusion**: The GDP-CO$_2$ relationship is complex and potentially non-stationary. 
A multi-country panel analysis would be necessary to characterize long-term decoupling.
"""))

else:
    # CASE 2: Valid calculation
    elasticity_LR = elasticity_SR / (1 - rho)
    
    effect_SR_pct = abs(elasticity_SR) * 100
    effect_LR_pct = abs(elasticity_LR) * 100
    verb_SR = "reduces" if elasticity_SR < 0 else "increases"
    verb_LR = "reduction" if elasticity_LR < 0 else "increase"
    
    halflife_approx = int(-np.log(0.5) / np.log(rho)) if 'HALFLIFE' not in locals() else int(HALFLIFE)
    years_LR = halflife_approx * 2
    
    display(Markdown(f"""
**US Average GDP**: ${gdp_mean_level:,}$ \$/capita (PPP 2017)

| Horizon | Elasticity | Interpretation |
|---------|-----------|----------------|
| **Short term** (year 1) | $\\varepsilon_{{ST}} = {elasticity_SR:.3f}$ | +1\\% GDP **{verb_SR}** emissions by **{effect_SR_pct:.2f}\\%** |
| **Long term** (~{years_LR} years) | $\\varepsilon_{{LT}} = {elasticity_LR:.3f}$ | Cumulative effect: **{verb_LR} of {effect_LR_pct:.2f}\\%** |

**Verdict**: {
    f"**Decoupling confirmed** (negative elasticity), but pace insufficient for Net Zero 2050." 
    if elasticity_LR < 0 
    else f"**Persistent coupling** â€” the decoupling observed in cross-section is not yet structural over time."
}
"""))
```

### Turning point

```{python}
#| echo: false

beta_pib = m_final.params['log_gdp_c']
beta_pib2 = m_final.params['I(log_gdp_c ** 2)']

if rho >= 0.99:
    display(Markdown("""
**Turning point: Not calculable** ($\\rho \\approx 1$ â†’ no defined long-term equilibrium)

**Empirical observation**: Peak ~2005-2007 (~20 t/capita), 30\\% drop since. 
But structural stability uncertain (cyclical or sustainable?).
"""))

elif beta_pib2 < 0:
    log_gdp_turning_c = -beta_pib / (2 * beta_pib2)  # Centered Turning point
    log_gdp_turning = log_gdp_turning_c + GDP_MEAN_USA  # De-centering
    gdp_turning = np.exp(log_gdp_turning)
    
    if 1000 < gdp_turning < 200000:
        gdp_usa_current = df_usa.iloc[-1]['gdp_pc']
        distance_pct = (gdp_usa_current - gdp_turning) / gdp_turning * 100
        
        display(Markdown(f"""
**Turning point**: ${int(gdp_turning):,}$ \$/capita | **US GDP 2020**: ${int(gdp_usa_current):,}$ \$/capita  
â†’ {"**Passed**" if gdp_usa_current > gdp_turning else "**Not reached**"} (${distance_pct:+.1f}$\\%)

{
    f"The USA is in the **downward phase** of the EKC. But this decoupling remains too slow for Paris 2030." 
    if gdp_usa_current > gdp_turning 
    else "Contradiction with data (drop since 2005) â†’ the model smoothes post-2010 breaks."
}
"""))
    else:
        display(Markdown(f"""
**Incoherent turning point** (${int(gdp_turning):,}$ \$/capita, out of realistic range 10kâ€“100k).  
**Empirical observation**: Peak ~2005-2007 (~45kâ€“50k \$/capita).
"""))

else:
    display(Markdown(f"""
**No turning point** ($\\beta_2 = {beta_pib2:.4f} \\geq 0$, no inverted U).  
Contradiction with observed data â†’ post-2010 decoupling is too recent for this global model.
"""))
```

## Projection 2021-2025 {#sec-projection-usa}

```{python}
#| label: projection-avec-ic
#| output: false

last_obs = df_usa.iloc[-1]
future_years = list(range(END_YEAR + 1, END_YEAR + 6))

future_df = []
curr_log_co2 = last_obs['log_co2']
curr_log_gdp = last_obs['log_gdp']

GROWTH_RATE_ANNUAL = 0.02

for year in future_years:
    curr_log_gdp += np.log(1 + GROWTH_RATE_ANNUAL) 
    curr_log_gdp_c = curr_log_gdp - GDP_MEAN_USA
    
    X_fut = pd.DataFrame({
        'lag_log_co2': [curr_log_co2],
        'log_gdp_c': [curr_log_gdp_c]
    })
    X_fut['I(log_gdp_c ** 2)'] = X_fut['log_gdp_c'] ** 2
    
    # Prediction with interval
    pred = m_final.get_prediction(X_fut)
    pred_summary = pred.summary_frame(alpha=0.05)
    
    future_df.append({
        'year': year,
        'log_co2_pred': pred_summary['mean'].values[0],
        'log_co2_lower': pred_summary['mean_ci_lower'].values[0],
        'log_co2_upper': pred_summary['mean_ci_upper'].values[0],
    })
    
    curr_log_co2 = pred_summary['mean'].values[0]

df_future = pd.DataFrame(future_df)
df_future['co2_pred'] = np.exp(df_future['log_co2_pred'])
df_future['co2_lower'] = np.exp(df_future['log_co2_lower'])
df_future['co2_upper'] = np.exp(df_future['log_co2_upper'])
```

```{python}
#| label: fig-projection-ic
#| fig-cap: "US Projection 2021-2025 with 95% Confidence Intervals"

fig, ax = plt.subplots(figsize=(12, 6))

# History
ax.plot(df_usa['year'], df_usa['co2_per_capita'], 
        'ko-', linewidth=2, markersize=6, label='History')

# Projection
ax.plot(df_future['year'], df_future['co2_pred'], 
        'r--', linewidth=2.5, marker='s', markersize=8, label='Projection (+2% GDP/year)')

# Confidence Interval
ax.fill_between(df_future['year'], 
                df_future['co2_lower'], 
                df_future['co2_upper'],
                color='red', alpha=0.2, label='95% CI')

ax.axvline(END_YEAR, color='gray', linestyle=':', linewidth=2)
ax.text(END_YEAR + 0.3, ax.get_ylim()[1]*0.95, 
        'Historical\nlimit', fontsize=9, va='top')

ax.set_xlabel("Year", fontsize=12)
ax.set_ylabel("COâ‚‚ per capita (tons)", fontsize=12)
ax.set_title("US Projection: Trend Scenario (+2% GDP/year)", fontsize=14, fontweight='bold')
ax.legend(fontsize=10)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

```{python}
#| echo: false

CO2_2020 = df_usa.iloc[-1]['co2_per_capita']
CO2_2025 = df_future.iloc[-1]['co2_pred']
variation = (CO2_2025 - CO2_2020) / CO2_2020 * 100

display(Markdown(f"""
**Projection table**:

| Year | Predicted COâ‚‚ | 95% CI | Cumulative Variation |
|:-----:|:----------:|:------:|:-----------------:|
""" + "\n".join([
    f"| {row['year']} | {row['co2_pred']:.2f} | [{row['co2_lower']:.2f} ; {row['co2_upper']:.2f}] | {(row['co2_pred'] - CO2_2020)/CO2_2020*100:+.1f}% |"
    for _, row in df_future.iterrows()
]) + f"""

**Key message**: Under the assumption of trend growth (+2%/year), US emissions 
{
    f"**would decrease by {abs(variation):.1f}%**" if variation < 0 
    else f"**would increase by {variation:.1f}%**"
} by 2025.

**Limitations of this projection**:

1. **Increasing uncertainty**: The confidence interval widens (Â±{(df_future.iloc[-1]['co2_upper'] - df_future.iloc[-1]['co2_lower'])/2:.1f} t in 2025)
2. **Structural stability hypothesis**: Assumes the GDP-COâ‚‚ relationship estimated over 1990-2020 remains valid
3. **No exogenous shocks**: Ignores future climate policies, crises, technological breakthroughs
4. **Single scenario**: A low-carbon scenario (+1% GDP, strong decarbonization) would yield very different results

**â†’ These projections are **conditional extrapolations**, not certain forecasts.**
"""))
```

## Summary of the temporal analysis

```{python}
#| echo: false

# 1. Intermediate calculations
if 'elasticity_LR' in locals() and not np.isnan(elasticity_LR):
    text_elasticity = f"{elasticity_LR:+.3f}"
    interp_elasticity = "Persistent coupling" if elasticity_LR > 0 else "Structurally confirmed decoupling"
else:
    text_elasticity = "Not calculable ($\\rho \\approx 1$)"
    interp_elasticity = "Complex/non-stationary relationship"

if 'HALFLIFE' in locals() and not np.isnan(HALFLIFE):
    text_halflife = f"{HALFLIFE:.1f} years"
    text_inertia = f"Decoupling is **slow**: a policy must be maintained > {int(HALFLIFE*2)} years"
else:
    text_halflife = "Not calculable ($\\rho \\approx 1$)"
    text_inertia = "Extreme persistence (quasi non-stationary process)"

if variation < 0 and variation > -20:
    interp_2025 = "Modest decrease, insufficient for Net Zero 2050"
elif variation > 0:
    interp_2025 = "Worrying increase, incompatible with Paris"
else:
    interp_2025 = "Significant decrease, compatible with transition"

if 'elasticity_LR' in locals() and elasticity_LR < 0:
    final_conclusion = "decoupling remains **too slow**"
else:
    final_conclusion = "the GDP-$\\text{CO}_2$ relationship is complex and unstable"

# 2. Display
display(Markdown(f"""
**Key results**:

1. **Retained model**: Quadratic ARDL  
   â†’ Cubic term not significant ($p = {m_ardl_cub.pvalues.get('I(log_gdp_c ** 3)', 1):.3f}$) 
   â†’ N-curve **not detected** over 1990-2020

2. **Structural inertia**: $\\rho = {rho:.3f}$, half-life = {text_halflife}  
   â†’ {text_inertia}

3. **Long-term elasticity**: {text_elasticity}  
   â†’ {interp_elasticity}

4. **Projection 2025**: {CO2_2025:.1f} $\\text{{tCO}}_2/\\text{{capita}}$ ({variation:+.1f}% vs 2020)  
   â†’ {interp_2025}

**â†’ The US trajectory does NOT show a rebound (N-curve), but {final_conclusion} 
to reach climate goals without major political disruption.**
"""))
```
